{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31cbfa8",
   "metadata": {},
   "source": [
    "# import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65be4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0fd2b",
   "metadata": {},
   "source": [
    "# read the data\n",
    "# convert texts to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e3fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=open('irish').read();\n",
    "# print(data)\n",
    "corpus=data.lower().split('\\n')\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "long=len(tokenizer.word_index)+1\n",
    "input_sequences=[]\n",
    "for item in corpus:\n",
    "    list_token = tokenizer.texts_to_sequences([item])[0]\n",
    "    for x in range(1, len(list_token)):\n",
    "        n_len=list_token[:x+1]\n",
    "        input_sequences.append(n_len);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea6d02",
   "metadata": {},
   "source": [
    "# - padding the sequences\n",
    "# - define the training data and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6676404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence=max([len(i) for i in input_sequences])\n",
    "input_padded=pad_sequences(input_sequences,padding='pre',maxlen=max_sequence)\n",
    "input_padded=np.array(input_padded)\n",
    "training_data=input_padded[:,:-1]\n",
    "labels_data=input_padded[:,-1]\n",
    "labels_data= tensorflow.keras.utils.to_categorical(labels_data,num_classes=long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b34e",
   "metadata": {},
   "source": [
    "# create the model using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da10a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    Embedding(long,100,input_length=max_sequence-1),\n",
    "    Bidirectional(LSTM(128,return_sequences=True)),\n",
    "    Bidirectional(LSTM(64,return_sequences=True)),\n",
    "    Bidirectional(LSTM(24)),\n",
    "    Dense(long,activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d64ff8",
   "metadata": {},
   "source": [
    "# compile and fiting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce06851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "377/377 [==============================] - 51s 96ms/step - loss: 5.2635 - accuracy: 0.1292\n",
      "Epoch 2/100\n",
      "377/377 [==============================] - 36s 95ms/step - loss: 5.0737 - accuracy: 0.1297\n",
      "Epoch 3/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 4.9998 - accuracy: 0.1317\n",
      "Epoch 4/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 4.9158 - accuracy: 0.1376\n",
      "Epoch 5/100\n",
      "377/377 [==============================] - 36s 96ms/step - loss: 4.8413 - accuracy: 0.1416\n",
      "Epoch 6/100\n",
      "377/377 [==============================] - 36s 95ms/step - loss: 4.7785 - accuracy: 0.1436\n",
      "Epoch 7/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 4.7185 - accuracy: 0.1472\n",
      "Epoch 8/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 4.7006 - accuracy: 0.1505\n",
      "Epoch 9/100\n",
      "377/377 [==============================] - 36s 96ms/step - loss: 4.6143 - accuracy: 0.1549\n",
      "Epoch 10/100\n",
      "377/377 [==============================] - 36s 96ms/step - loss: 4.5718 - accuracy: 0.1568\n",
      "Epoch 11/100\n",
      "377/377 [==============================] - 37s 97ms/step - loss: 4.4947 - accuracy: 0.1603\n",
      "Epoch 12/100\n",
      "377/377 [==============================] - 37s 97ms/step - loss: 4.4441 - accuracy: 0.1656\n",
      "Epoch 13/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 4.3842 - accuracy: 0.1711\n",
      "Epoch 14/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 4.3257 - accuracy: 0.1763\n",
      "Epoch 15/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 4.2770 - accuracy: 0.1779\n",
      "Epoch 16/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 4.2217 - accuracy: 0.1828\n",
      "Epoch 17/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 4.1597 - accuracy: 0.1882\n",
      "Epoch 18/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 4.1115 - accuracy: 0.1921\n",
      "Epoch 19/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 4.0648 - accuracy: 0.1973\n",
      "Epoch 20/100\n",
      "377/377 [==============================] - 37s 97ms/step - loss: 4.0183 - accuracy: 0.2029\n",
      "Epoch 21/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 3.9577 - accuracy: 0.2093\n",
      "Epoch 22/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 3.9129 - accuracy: 0.2130\n",
      "Epoch 23/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 3.8659 - accuracy: 0.2214\n",
      "Epoch 24/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.8250 - accuracy: 0.2233\n",
      "Epoch 25/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.7718 - accuracy: 0.2275\n",
      "Epoch 26/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.7375 - accuracy: 0.2363\n",
      "Epoch 27/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.6895 - accuracy: 0.2388\n",
      "Epoch 28/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 3.6303 - accuracy: 0.2504\n",
      "Epoch 29/100\n",
      "377/377 [==============================] - 39s 104ms/step - loss: 3.5821 - accuracy: 0.2536\n",
      "Epoch 30/100\n",
      "377/377 [==============================] - 40s 106ms/step - loss: 3.5312 - accuracy: 0.2638\n",
      "Epoch 31/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 3.5069 - accuracy: 0.2667\n",
      "Epoch 32/100\n",
      "377/377 [==============================] - 37s 98ms/step - loss: 3.4484 - accuracy: 0.2754\n",
      "Epoch 33/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 3.3957 - accuracy: 0.2845\n",
      "Epoch 34/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.3470 - accuracy: 0.2899\n",
      "Epoch 35/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.3214 - accuracy: 0.2932\n",
      "Epoch 36/100\n",
      "377/377 [==============================] - 39s 102ms/step - loss: 3.2675 - accuracy: 0.3060\n",
      "Epoch 37/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 3.2108 - accuracy: 0.3175\n",
      "Epoch 38/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.1697 - accuracy: 0.3247\n",
      "Epoch 39/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.1182 - accuracy: 0.3349\n",
      "Epoch 40/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 3.0923 - accuracy: 0.3423\n",
      "Epoch 41/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 3.0525 - accuracy: 0.3432\n",
      "Epoch 42/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 3.0036 - accuracy: 0.3540\n",
      "Epoch 43/100\n",
      "377/377 [==============================] - 39s 102ms/step - loss: 2.9677 - accuracy: 0.3629\n",
      "Epoch 44/100\n",
      "377/377 [==============================] - 39s 104ms/step - loss: 2.9134 - accuracy: 0.3742\n",
      "Epoch 45/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 2.8750 - accuracy: 0.3800\n",
      "Epoch 46/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 2.8346 - accuracy: 0.3890\n",
      "Epoch 47/100\n",
      "377/377 [==============================] - 39s 104ms/step - loss: 2.7981 - accuracy: 0.3933\n",
      "Epoch 48/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 2.7611 - accuracy: 0.4055\n",
      "Epoch 49/100\n",
      "377/377 [==============================] - 39s 102ms/step - loss: 2.7162 - accuracy: 0.4134\n",
      "Epoch 50/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 2.6817 - accuracy: 0.4219\n",
      "Epoch 51/100\n",
      "377/377 [==============================] - 38s 102ms/step - loss: 2.6532 - accuracy: 0.4262\n",
      "Epoch 52/100\n",
      "377/377 [==============================] - 38s 102ms/step - loss: 2.6055 - accuracy: 0.4347\n",
      "Epoch 53/100\n",
      "377/377 [==============================] - 40s 107ms/step - loss: 2.5897 - accuracy: 0.4403\n",
      "Epoch 54/100\n",
      "377/377 [==============================] - 39s 102ms/step - loss: 2.5531 - accuracy: 0.4453\n",
      "Epoch 55/100\n",
      "377/377 [==============================] - 41s 108ms/step - loss: 2.5140 - accuracy: 0.4566\n",
      "Epoch 56/100\n",
      "377/377 [==============================] - 40s 106ms/step - loss: 2.4633 - accuracy: 0.4656\n",
      "Epoch 57/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 2.4124 - accuracy: 0.4778\n",
      "Epoch 58/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 2.3966 - accuracy: 0.4827\n",
      "Epoch 59/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 2.3557 - accuracy: 0.4902\n",
      "Epoch 60/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 2.3219 - accuracy: 0.5024\n",
      "Epoch 61/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 2.2792 - accuracy: 0.5041\n",
      "Epoch 62/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 2.2427 - accuracy: 0.5208\n",
      "Epoch 63/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 2.2423 - accuracy: 0.5169\n",
      "Epoch 64/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 2.1870 - accuracy: 0.5280\n",
      "Epoch 65/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 2.1530 - accuracy: 0.5372\n",
      "Epoch 66/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 2.1148 - accuracy: 0.5451\n",
      "Epoch 67/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 2.0768 - accuracy: 0.5567\n",
      "Epoch 68/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 2.0577 - accuracy: 0.5580\n",
      "Epoch 69/100\n",
      "377/377 [==============================] - 40s 106ms/step - loss: 2.0253 - accuracy: 0.5665\n",
      "Epoch 70/100\n",
      "377/377 [==============================] - 41s 108ms/step - loss: 2.0189 - accuracy: 0.5638\n",
      "Epoch 71/100\n",
      "377/377 [==============================] - 40s 105ms/step - loss: 1.9727 - accuracy: 0.5762\n",
      "Epoch 72/100\n",
      "377/377 [==============================] - 39s 102ms/step - loss: 1.9399 - accuracy: 0.5835\n",
      "Epoch 73/100\n",
      "377/377 [==============================] - 42s 111ms/step - loss: 1.8923 - accuracy: 0.5959\n",
      "Epoch 74/100\n",
      "377/377 [==============================] - 39s 104ms/step - loss: 1.8780 - accuracy: 0.5984\n",
      "Epoch 75/100\n",
      "377/377 [==============================] - 40s 105ms/step - loss: 1.8605 - accuracy: 0.5982\n",
      "Epoch 76/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 1.8263 - accuracy: 0.6104\n",
      "Epoch 77/100\n",
      "377/377 [==============================] - 38s 100ms/step - loss: 1.8161 - accuracy: 0.6106\n",
      "Epoch 78/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 1.7700 - accuracy: 0.6237\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377/377 [==============================] - 38s 102ms/step - loss: 1.7346 - accuracy: 0.6308\n",
      "Epoch 80/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 1.7125 - accuracy: 0.6358\n",
      "Epoch 81/100\n",
      "377/377 [==============================] - 39s 102ms/step - loss: 1.7104 - accuracy: 0.6367\n",
      "Epoch 82/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 1.6702 - accuracy: 0.6468\n",
      "Epoch 83/100\n",
      "377/377 [==============================] - 38s 102ms/step - loss: 1.6555 - accuracy: 0.6435\n",
      "Epoch 84/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 1.6332 - accuracy: 0.6480\n",
      "Epoch 85/100\n",
      "377/377 [==============================] - 38s 101ms/step - loss: 1.6055 - accuracy: 0.6602\n",
      "Epoch 86/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 1.5675 - accuracy: 0.6724\n",
      "Epoch 87/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 1.5480 - accuracy: 0.6743\n",
      "Epoch 88/100\n",
      "377/377 [==============================] - 39s 104ms/step - loss: 1.5379 - accuracy: 0.6780\n",
      "Epoch 89/100\n",
      "377/377 [==============================] - 39s 104ms/step - loss: 1.5110 - accuracy: 0.6834\n",
      "Epoch 90/100\n",
      "377/377 [==============================] - 39s 103ms/step - loss: 1.4847 - accuracy: 0.6901\n",
      "Epoch 91/100\n",
      "377/377 [==============================] - 37s 99ms/step - loss: 1.4854 - accuracy: 0.6851\n",
      "Epoch 92/100\n",
      "377/377 [==============================] - 38s 102ms/step - loss: 1.4546 - accuracy: 0.6946\n",
      "Epoch 93/100\n",
      "377/377 [==============================] - 40s 107ms/step - loss: 1.4234 - accuracy: 0.7002\n",
      "Epoch 94/100\n",
      "377/377 [==============================] - 41s 108ms/step - loss: 1.4102 - accuracy: 0.7054\n",
      "Epoch 95/100\n",
      "377/377 [==============================] - 41s 109ms/step - loss: 1.4234 - accuracy: 0.6976\n",
      "Epoch 96/100\n",
      "377/377 [==============================] - 41s 108ms/step - loss: 1.3765 - accuracy: 0.7117\n",
      "Epoch 97/100\n",
      "377/377 [==============================] - 41s 110ms/step - loss: 1.3487 - accuracy: 0.7174\n",
      "Epoch 98/100\n",
      "377/377 [==============================] - 42s 112ms/step - loss: 1.3205 - accuracy: 0.7275\n",
      "Epoch 99/100\n",
      "377/377 [==============================] - 41s 109ms/step - loss: 1.3174 - accuracy: 0.7224\n",
      "Epoch 100/100\n",
      "377/377 [==============================] - 40s 105ms/step - loss: 1.2924 - accuracy: 0.7282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b218405e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(training_data,labels_data,epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1436d7ca",
   "metadata": {},
   "source": [
    "# make prediction and test on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef821a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\n",
      "my dog loves to\n",
      " not\n",
      " be\n",
      " thee\n",
      " dear\n",
      " in\n",
      " home\n",
      " on\n",
      " gilgarra\n",
      " mountain\n",
      " thyme\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_text=\"my dog loves to\" \n",
    "out_data=\"\"\n",
    "next_word=10\n",
    "for _ in range(next_word):\n",
    "        seed_seq=tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        seed_padded=pad_sequences([seed_seq],maxlen=max_sequence-1,padding='pre')\n",
    "        rslt=np.argmax(model.predict([seed_padded]))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if  index==rslt:\n",
    "                out_data=word\n",
    "                seed_text+=\"\\n \" + out_data\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc17c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
